{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('C:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-18-2dfc28fca47d>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-2dfc28fca47d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-18-2dfc28fca47d>:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3389.socketTextStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after stopping a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:228)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:66)\r\n\tat org.apache.spark.streaming.dstream.InputDStream.<init>(InputDStream.scala:45)\r\n\tat org.apache.spark.streaming.dstream.ReceiverInputDStream.<init>(ReceiverInputDStream.scala:42)\r\n\tat org.apache.spark.streaming.dstream.SocketInputDStream.<init>(SocketInputDStream.scala:40)\r\n\tat org.apache.spark.streaming.StreamingContext.socketStream(StreamingContext.scala:322)\r\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$socketTextStream$1.apply(StreamingContext.scala:303)\r\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$socketTextStream$1.apply(StreamingContext.scala:303)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.streaming.StreamingContext.withNamedScope(StreamingContext.scala:274)\r\n\tat org.apache.spark.streaming.StreamingContext.socketTextStream(StreamingContext.scala:302)\r\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.socketTextStream(JavaStreamingContext.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-709aafd60e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msocket_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocketTextStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"127.0.0.1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5555\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\streaming\\context.py\u001b[0m in \u001b[0;36msocketTextStream\u001b[1;34m(self, hostname, port, storageLevel)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \"\"\"\n\u001b[0;32m    254\u001b[0m         \u001b[0mjlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getJavaStorageLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         return DStream(self._jssc.socketTextStream(hostname, port, jlevel), self,\n\u001b[0m\u001b[0;32m    256\u001b[0m                        UTF8Deserializer())\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3389.socketTextStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after stopping a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:228)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:66)\r\n\tat org.apache.spark.streaming.dstream.InputDStream.<init>(InputDStream.scala:45)\r\n\tat org.apache.spark.streaming.dstream.ReceiverInputDStream.<init>(ReceiverInputDStream.scala:42)\r\n\tat org.apache.spark.streaming.dstream.SocketInputDStream.<init>(SocketInputDStream.scala:40)\r\n\tat org.apache.spark.streaming.StreamingContext.socketStream(StreamingContext.scala:322)\r\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$socketTextStream$1.apply(StreamingContext.scala:303)\r\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$socketTextStream$1.apply(StreamingContext.scala:303)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.streaming.StreamingContext.withNamedScope(StreamingContext.scala:274)\r\n\tat org.apache.spark.streaming.StreamingContext.socketTextStream(StreamingContext.scala:302)\r\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.socketTextStream(JavaStreamingContext.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.streaming.api.python.PythonTransformedDStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after stopping a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:228)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:66)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream.<init>(PythonDStream.scala:224)\r\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.<init>(PythonDStream.scala:241)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4e1dc83c5bb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# Reduces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTweet\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# Stores in a Tweet Object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m   .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n\u001b[0m\u001b[0;32m      7\u001b[0m   .limit(10).registerTempTable(\"tweets\") ) ) # Registers to a table.\n",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\streaming\\dstream.py\u001b[0m in \u001b[0;36mforeachRDD\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mjfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonDStream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallForeachRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\streaming\\dstream.py\u001b[0m in \u001b[0;36m_jdstream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[0mjfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mdstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonTransformedDStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdstream_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masJavaDStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdstream_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1525\u001b[1;33m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[0;32m   1526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.streaming.api.python.PythonTransformedDStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after stopping a context is not supported\r\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:228)\r\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:66)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream.<init>(PythonDStream.scala:224)\r\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.<init>(PythonDStream.scala:241)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") ) # Checks for hashtag calls\n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) # Reduces\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) ) # Stores in a Tweet Object\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
    "  .limit(10).registerTempTable(\"tweets\") ) ) # Registers to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3389.start.\n: java.lang.IllegalStateException: StreamingContext has already been stopped\r\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:608)\r\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9eb6e7f74a80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\streaming\\context.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \"\"\"\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/Users/HP/Desktop/spark/spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3389.start.\n: java.lang.IllegalStateException: StreamingContext has already been stopped\r\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:608)\r\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure( figsize = ( 10, 8 ) )\n",
    "    sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
